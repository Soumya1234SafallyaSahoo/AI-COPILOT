# ğŸš€ 30 Days of AI Voice Agents Challenge â€“ Day 13: Documentation

# ğŸ“– Introduction
The **AI Voice Agent** is an intelligent, interactive voice-based assistant that listens, understands, and responds in real time.  
It combines **speech recognition**, **natural language processing**, and **text-to-speech** to create a smooth and human-like conversational experience.  

ğŸ’¡ This project is built as part of the **30 Days of AI Voice Agents Challenge**.

# ğŸ“ About the Project
This project aims to create a **full-stack AI-powered voice assistant** that works directly in the browser without extra installations.  

## ğŸ” How it Works
1. **User Speaks** ğŸ¤ â€“ The browser records the voice.
2. **Speech-to-Text** ğŸ“ â€“ Audio is sent to the backend, transcribed using AssemblyAI.
3. **AI Processing** ğŸ¤– â€“ The transcribed text is processed using a Large Language Model (LLM) via the OpenAI API.
4. **Text-to-Speech** ğŸ”Š â€“ AIâ€™s response is converted back into audio.
5. **User Hears Response** ğŸ‘‚ â€“ Played instantly in the browser.

This modular design makes it perfect for **customer support**, **virtual teaching assistants**, or **voice-enabled applications**.

# ğŸš€ Key Features
- ğŸ¤ **Voice Input** â€“ Record audio from your browser with one click.
- ğŸ§  **AI-Powered Responses** â€“ Generates smart, context-aware replies.
- ğŸ¯ **Accurate Transcription** â€“ High-accuracy Speech-to-Text via AssemblyAI.
- ğŸ”Š **Natural Voice Output** â€“ Instant text-to-speech playback.
- ğŸŒ **Web-Based Interface** â€“ Works directly in the browser.
- âš¡ **Real-Time Performance** â€“ Designed for speed and responsiveness.

---

# ğŸ›  Tech Stack
| ğŸ’¡ Component        | ğŸ”§ Technology Used |
|---------------------|--------------------|
| Backend API         | FastAPI (Python) |
| Frontend UI         | HTML, CSS, JavaScript |
| Speech-to-Text      | AssemblyAI API |
| AI Processing       | OpenAI API |
| Text-to-Speech      | gTTS / Murf.ai |
| Server Hosting      | Uvicorn |

# ğŸ— Architecture

The AI Voice Agent follows a **client-server architecture** with API-based integrations for transcription, AI processing, and text-to-speech.

**Workflow:**
1. **User Speaks** ğŸ¤ â€“ Audio is captured from the browser using JavaScript.
2. **Frontend Sends Audio** ğŸŒ â€“ Audio file is sent to the FastAPI backend.
3. **Speech-to-Text** ğŸ“ â€“ AssemblyAI API converts the audio to text.
4. **AI Processing** ğŸ¤– â€“ OpenAI API processes the text and generates a smart response.
5. **Text-to-Speech** ğŸ”Š â€“ gTTS or Murf.ai converts the response into audio.
6. **User Hears Response** ğŸ‘‚ â€“ The frontend plays the generated audio in real time.
7. # âš™ï¸ Getting Started

---

## ğŸ“‹ Prerequisites
Before running the project, make sure you have:

- ğŸ **Python 3.9+** installed  
- ğŸ“¦ **pip** (Python package manager)  
- ğŸŒ Stable internet connection  
- ğŸ”‘ API keys for:
  - **AssemblyAI** (Speech-to-Text)
  - **OpenAI** (LLM Processing)
  - **Murf.ai** (Text-to-Speech)
- ğŸ’» Any modern web browser (Chrome, Edge, Firefox, etc.)






